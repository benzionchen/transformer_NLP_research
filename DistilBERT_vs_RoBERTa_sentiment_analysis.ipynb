{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/benzionchen/transformer_NLP_research/blob/main/transformer_research_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wp1Z6WIbjyIN"
   },
   "source": [
    "The goal is for you to practice:\n",
    "\n",
    "- Picking a dataset (and possibly trying more than one).\n",
    "- Selecting different pretrained models from the Hugging Face Hub.\n",
    "- Measuring performance (accuracy, F1, etc.) and comparing your results.\n",
    "- Doing a brief error analysis to see where each model struggles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "UAvwaCV3mEK9",
    "outputId": "a1db1fed-9b18-478c-d516-38775b5f47cc"
   },
   "source": [
    "Already installed:\n",
    "\n",
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install evaluate\n",
    "!pip install accelerate\n",
    "!pip install -U bitsandbytes\n",
    "!pip install torch\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEic4hEku7h0"
   },
   "source": [
    "## 1. Choose a Dataset\n",
    "You can pick: (or try some other ones you find interesting):\n",
    "- IMDB Movie Reviews (sentiment labels: positive/negative).\n",
    "- Yelp Reviews (sentiment labels: star ratings or binary positive/negative).\n",
    "\n",
    "For now, complete the rest of the steps (2-4) below with the above two datasets. Come back to do the following task after you're done with the above (time permitting):\n",
    "\n",
    "We are going to use the Amazon Product Reviews (various categories, can be collapsed into positive/negative) dataset. You are free to decide how to collapse multiple categories into one. You can also compare different approaches of this as well.\n",
    "\n",
    "Feel free to use the datasets library (e.g., load_dataset(\"imdb\")).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dlwIqTopjyhu",
    "outputId": "29fdcc07-b244-48f8-b9f8-eb90b3cfb523"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n",
      "I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn't match the background, and painfully one-dimensional characters cannot be overcome with a 'sci-fi' setting. (I'm sure there are those of you out there who think Babylon 5 is good sci-fi TV. It's not. It's clichéd and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It's really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it's rubbish as they have to always say \"Gene Roddenberry's Earth...\" otherwise people would not continue watching. Roddenberry's ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.\n",
      "0\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 560000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 38000\n",
      "    })\n",
      "})\n",
      "Contrary to other reviews, I have zero complaints about the service or the prices. I have been getting tire service here for the past 5 years now, and compared to my experience with places like Pep Boys, these guys are experienced and know what they're doing. \\nAlso, this is one place that I do not feel like I am being taken advantage of, just because of my gender. Other auto mechanics have been notorious for capitalizing on my ignorance of cars, and have sucked my bank account dry. But here, my service and road coverage has all been well explained - and let up to me to decide. \\nAnd they just renovated the waiting room. It looks a lot better than it did in previous years.\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "imdb = load_dataset(\"imdb\")\n",
    "yelp = load_dataset(\"yelp_polarity\") # the name of the dataset is not 'yelp', it's 'yelp_polarity'\n",
    "\n",
    "imdb_texts = imdb[\"test\"][\"text\"]\n",
    "imdb_labels = imdb[\"test\"][\"label\"]\n",
    "\n",
    "yelp_texts = yelp[\"test\"][\"text\"]\n",
    "yelp_labels = yelp[\"test\"][\"label\"]\n",
    "\n",
    "print(imdb)\n",
    "print(imdb_texts[0])\n",
    "print(imdb_labels[0])\n",
    "print(yelp)\n",
    "print(yelp_texts[0])\n",
    "print(yelp_labels[0])\n",
    "\n",
    "# /usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning:\n",
    "# The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
    "# To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
    "# You will be able to reuse this secret in all of your notebooks.\n",
    "\n",
    "# what does this mean? API key?\n",
    "# the dataset is quite big so we should limit it to maybe 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1Ov5zMtPGHLx"
   },
   "outputs": [],
   "source": [
    "imdb_texts = imdb[\"test\"][\"text\"][:1000]\n",
    "imdb_labels = imdb[\"test\"][\"label\"][:1000]\n",
    "\n",
    "yelp_texts = yelp[\"test\"][\"text\"][:1000]\n",
    "yelp_labels = yelp[\"test\"][\"label\"][:1000]\n",
    "\n",
    "# choose sample 1000 because original datasets are too big \n",
    "# 1000 is a good number, not too small and doesnt run into long runtime + prevent GPU memory overflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHuMnpeFvDSY"
   },
   "source": [
    "## 2. Select Two (or More) Pretrained Models\n",
    "\n",
    "Pick at least two from the Hugging Face Hub and compare them:\n",
    "\n",
    "DistilBERT (e.g., distilbert-base-uncased-finetuned-sst-2-english)\n",
    "\n",
    "BERT (e.g., bert-base-uncased-finetuned-sst-2-english)\n",
    "\n",
    "RoBERTa (e.g., cardiffnlp/twitter-roberta-base-sentiment-latest or roberta-base-openai-detector)\n",
    "\n",
    "Feel free to explore the Hugging Face Model Hub if you find something else interesting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 669
    },
    "id": "D50BX2QAvGAB",
    "outputId": "c4e8a881-8552-48ea-ef12-87a79fe9a49c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# load models\n",
    "model1 = pipeline(\n",
    "    'sentiment-analysis', \n",
    "    model = 'distilbert-base-uncased-finetuned-sst-2-english', \n",
    "    truncation = True # added truncation because runtime error\n",
    "    )\n",
    "\n",
    "model2 = pipeline(\n",
    "    'sentiment-analysis', \n",
    "    model = 'cardiffnlp/twitter-roberta-base-sentiment-latest',\n",
    "    truncation = True, # added truncation because runtime error\n",
    "    padding = True, # added padding due to error with different lengths \n",
    "    max_length = 512 # ensure sequences are no longer than 512 tokens w/ Deepseek (but no longer using DeepSeek since using RoBERTa)\n",
    "    )\n",
    "\n",
    "# migrated from colab -> jupyter notebook\n",
    "# bert-base-uncased-finetuned-sst-2-english is not available on HF? neither is \n",
    "# \"bert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "# in case of truncation, if =false, then you pick max length of dataset (which is # of cols in matrix)\n",
    "# if padding = false, (padding adds 0 or padding token), when passing batch, dependign on how model is \n",
    "# trained, can mess up model and shoudl really only use this when we need to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### method 2 (how chat GPT suggested to load the model)\n",
    "model1 = pipeline(\n",
    "    'sentiment-analysis', \n",
    "    model='distilbert-base-uncased-finetuned-sst-2-english',\n",
    "    tokenizer='distilbert-base-uncased-finetuned-sst-2-english', # what is the suggested tokenizer for? \n",
    "    device=0 # what is device=0 for? \n",
    ")\n",
    "\n",
    "model2 = pipeline(\n",
    "    'sentiment-analysis',\n",
    "    model='cardiffnlp/twitter-roberta-base-sentiment-latest',\n",
    "    tokenizer='cardiffnlp/twitter-roberta-base-sentiment-latest',\n",
    "    device=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gC8TTPHN6QBB",
    "outputId": "3d88163a-ebf3-445b-8a36-0c550ae31ef2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.pipelines.text_classification.TextClassificationPipeline object at 0x0000029404E6E040>\n",
      "<transformers.pipelines.text_classification.TextClassificationPipeline object at 0x0000029404FC3340>\n",
      "[{'label': 'POSITIVE', 'score': 0.999713122844696}]\n",
      "[{'label': 'NEGATIVE', 'score': 0.9991129040718079}]\n",
      "[{'label': 'positive', 'score': 0.9173767566680908}]\n",
      "[{'label': 'negative', 'score': 0.7887632846832275}]\n"
     ]
    }
   ],
   "source": [
    "# testing it's loaded\n",
    "print(model1)\n",
    "print(model2)\n",
    "print(model1(\"i love dogs\"))\n",
    "print(model1(\"i hate you\"))\n",
    "print(model2(\"i love dogs\"))\n",
    "print(model2(\"i hate you\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N9-jhwtIA0oS",
    "outputId": "490e5e2a-1b64-4b61-adbe-6358dd80a6fe",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2002054.42it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1999191.61it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2004925.43it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2006844.02it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# import a progress bar\n",
    "\n",
    "# use BERT to predict sentiment analysis for both imdb and yelp datasets\n",
    "imdb_prediction1 = [1 if r[\"label\"] == \"POSITIVE\"\n",
    "    else 0 for r in tqdm(model1(imdb_texts, batch_size=32, truncation=True, padding=True))]\n",
    "\n",
    "yelp_prediction1 = [1 if r[\"label\"] == \"POSITIVE\"\n",
    "    else 0 for r in tqdm(model1(yelp_texts, batch_size=32, truncation=True, padding=True))]\n",
    "\n",
    "# RuntimeError: The size of tensor a (532) must match the size of tensor b (512) at non-singleton dimension 1, need to add truncation for model1 (BERT)\n",
    "# this takes forever to tokenize if doing linearly 1 by 1, maybe can batch by 32, and 16 if run into memory problems\n",
    "\n",
    "imdb_prediction2 = [1 if r[\"label\"] == \"positive\"\n",
    "    else 0 for r in tqdm(model2(imdb_texts, batch_size=32, truncation=True, padding=True))]\n",
    "\n",
    "yelp_prediction2 = [1 if r[\"label\"] == \"positive\"\n",
    "    else 0 for r in tqdm(model2(yelp_texts, batch_size=32, truncation=True, padding=True))]\n",
    "\n",
    "# batch size = 32 is 32 sentences/predictions at once, nothing to with model, this is parallelism with \n",
    "# 1 batch = matrix with dimensions of (batch_size, sentence_length) - (row, cols)\n",
    "# size = max # of tokens sent (if have memory in GPU, can do all these predictions at once) \n",
    "# batch norm is different- training thing - normalize the batch (batch needs to be balanced in training\n",
    "# outlier so you can \"center\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "26nntL6bHb5P",
    "outputId": "c6627cd9-dfcd-474c-b906-33b6675ee932",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0]\n",
      "\n",
      "\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(imdb_prediction1)\n",
    "print(yelp_prediction1)\n",
    "print('\\n')\n",
    "print(len(imdb_prediction1))\n",
    "\n",
    "# 1000 negative and positive reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0]\n",
      "[1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0]\n",
      "\n",
      "\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "# check if yelp_prediction1 = yelp_prediction2, if not, compare the differences \n",
    "print(imdb_prediction2)\n",
    "print(yelp_prediction2)\n",
    "print('\\n')\n",
    "print(len(imdb_prediction2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_mismatches(array1, array2):\n",
    "    if len(array1) != len(array2):\n",
    "        raise ValueError(\"arrays must have the same length\")\n",
    "\n",
    "    # count mismatches\n",
    "    mismatch_count = sum(1 for a, b in zip(array1, array2) if a != b)\n",
    "\n",
    "    return mismatch_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162\n",
      "0.162\n"
     ]
    }
   ],
   "source": [
    "print(count_mismatches(imdb_prediction1, imdb_prediction2))\n",
    "print(count_mismatches(imdb_prediction1, imdb_prediction2)/1000)\n",
    "# 162 mismatches\n",
    "# 16.2 percent of mismatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VotMdHRgvGvp"
   },
   "source": [
    "## 3. Measure Performance\n",
    "Implement an evaluation method on a test or validation split. At minimum:\n",
    "- Accuracy: The fraction of examples predicted correctly.\n",
    "\n",
    "- F1 Score: Combination of precision and recall. (explanation of this is given below, after the instructions)\n",
    "You can use the Hugging Face evaluate or datasets library or write your own small function for computing these metrics.\n",
    "\n",
    "### F1 Score Explanation\n",
    "\n",
    "Imagine you’re trying to detect “positive” cases—for example, emails that are spam. Your model’s predictions might fall into these categories:\n",
    "- True Positive (TP): Predicted spam, actually spam\n",
    "- False Positive (FP): Predicted spam, but it’s not spam\n",
    "- True Negative (TN): Predicted not spam, actually not spam\n",
    "- False Negative (FN): Predicted not spam, but it was spam\n",
    "\n",
    "Two important measures come from this:\n",
    "- Precision: Out of the emails you labeled spam, how many were actually spam? Precision=TP/(TP + FP)​\n",
    "- Recall: Out of the emails that were actually spam, how many did you catch? Recall=TP​/(TP + FN)\n",
    "\n",
    "But often, focusing on just Precision or just Recall is not enough. The F1 score combines both in a single number. It’s defined as the harmonic mean of Precision and Recall:\n",
    "\n",
    "F1=2× ((Precision×Recall​)/(Precision+Recall))\n",
    "\n",
    "This way, if either Precision or Recall is low, the F1 score will also be relatively low.\n",
    "\n",
    "Example with a Small Confusion Matrix\n",
    "Suppose your model had these results:\n",
    "\n",
    "- TP = 4\n",
    "- FP = 2\n",
    "- FN = 1\n",
    "- TN = 3\n",
    "\n",
    "Then:\n",
    "\n",
    "- Precision=4/(4+2)​=0.66 (about 66%)\n",
    "- Recall=4/(4+1)​=0.80 (80%)\n",
    "\n",
    "So,\n",
    "F1=2×(0.66×0.80)/(0.66+0.80)​≈0.72\n",
    "\n",
    "Quick Python Example\n",
    "\n",
    "Below is a short snippet using sklearn (you dont have to usethis, hugging face also has a f1 function) to calculate the F1 score from some example predictions:\n",
    "\n",
    "\n",
    "```\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "True labels and model predictions\n",
    "y_true = [1, 1, 0, 1, 0, 0]\n",
    "y_pred = [1, 0, 0, 1, 0, 1]\n",
    "\n",
    "Calculate F1 score\n",
    "score = f1_score(y_true, y_pred)\n",
    "print(\"F1 Score:\", score)\n",
    "```\n",
    "\n",
    "If you run this, you’ll see a single value that summarizes how good your predictions are at correctly identifying positives (with both “how often you’re correct” in positives, and “how many positives you caught” taken into account).\n",
    "\n",
    "Key Takeaways\n",
    "\n",
    "- F1 score balances Precision and Recall into one number.\n",
    "- If you need a single metric to judge performance in situations where both false positives and false negatives matter, F1 is often a good choice.\n",
    "- In Python, `sklearn.metrics.f1_score` makes it easy to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9d4tIeYy5rrk",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBERT IMDB Accuracy: 0.908, F1: 0.0\n",
      "DistilBERT Yelp Accuracy: 0.891, F1: 0.883919062832801\n",
      "RoBERTa IMDB Accuracy: 0.888, F1: 0.0\n",
      "RoBERTa Yelp Accuracy: 0.858, F1: 0.8582834331337326\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "\n",
    "def calculate_performance(predictions, labels):\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions)  \n",
    "    return accuracy, f1\n",
    "\n",
    "distilbert_imdb_accuracy, distilbert_imdb_f1 = calculate_performance(imdb_prediction1, imdb_labels)\n",
    "distilbert_yelp_accuracy, distilbert_yelp_f1 = calculate_performance(yelp_prediction1, yelp_labels)\n",
    "roberta_imdb_accuracy, roberta_imdb_f1 = calculate_performance(imdb_prediction2, imdb_labels)\n",
    "roberta_yelp_accuracy, roberta_yelp_f1 = calculate_performance(yelp_prediction2, yelp_labels)\n",
    "\n",
    "print(f\"DistilBERT IMDB Accuracy: {distilbert_imdb_accuracy}, F1: {distilbert_imdb_f1}\")\n",
    "print(f\"DistilBERT Yelp Accuracy: {distilbert_yelp_accuracy}, F1: {distilbert_yelp_f1}\")\n",
    "print(f\"RoBERTa IMDB Accuracy: {roberta_imdb_accuracy}, F1: {roberta_imdb_f1}\")\n",
    "print(f\"RoBERTa Yelp Accuracy: {roberta_yelp_accuracy}, F1: {roberta_yelp_f1}\")\n",
    "\n",
    "# F1 is producing 0? why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DistilBERT IMDB: \n",
    "- model is performing well on the negative class (precision: 1.00, recall: 0.91, F1: 0.95), the recall and f1-score are zero, which means DistilBERT is failing to predict any positive instances\n",
    "\n",
    "RoBERTa IMDB: \n",
    "- performs very well on the negative class (precision: 1.00, recall: 0.89, F1: 0.94), but fails to predict any positive instances (F1: 0.0)\n",
    "\n",
    "\n",
    "Need to fix this problem with IMDB dataset positive instances not getting detected\n",
    "\n",
    "\n",
    "DistilBERT Yelp:  \n",
    "- model is doing better on Yelp with both positive and negative classes showing reasonable performance (precision, recall, and F1 around 0.88 to 0.91), and  identifying positive and negative reviews \n",
    "\n",
    "RoBERTa Yelp: \n",
    "- lower performance on Yelp (accuracy: 0.86, F1: 0.86), but it's still within range, not as strong as DistilBERT on this dataset but performs decently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBERT Raw Output (first 10 examples):\n",
      "Text: I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn't match the background, and painfully one-dimensional characters cannot be overcome with a 'sci-fi' setting. (I'm sure there are those of you out there who think Babylon 5 is good sci-fi TV. It's not. It's clichéd and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It's really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it's rubbish as they have to always say \"Gene Roddenberry's Earth...\" otherwise people would not continue watching. Roddenberry's ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.\n",
      "\n",
      "\n",
      "Prediction: NEGATIVE, Confidence: 0.999616265296936\n",
      "\n",
      "\n",
      "Text: Worth the entertainment value of a rental, especially if you like action movies. This one features the usual car chases, fights with the great Van Damme kick style, shooting battles with the 40 shell load shotgun, and even terrorist style bombs. All of this is entertaining and competently handled but there is nothing that really blows you away if you've seen your share before.<br /><br />The plot is made interesting by the inclusion of a rabbit, which is clever but hardly profound. Many of the characters are heavily stereotyped -- the angry veterans, the terrified illegal aliens, the crooked cops, the indifferent feds, the bitchy tough lady station head, the crooked politician, the fat federale who looks like he was typecast as the Mexican in a Hollywood movie from the 1940s. All passably acted but again nothing special.<br /><br />I thought the main villains were pretty well done and fairly well acted. By the end of the movie you certainly knew who the good guys were and weren't. There was an emotional lift as the really bad ones got their just deserts. Very simplistic, but then you weren't expecting Hamlet, right? The only thing I found really annoying was the constant cuts to VDs daughter during the last fight scene.<br /><br />Not bad. Not good. Passable 4.\n",
      "\n",
      "\n",
      "Prediction: NEGATIVE, Confidence: 0.6170620322227478\n",
      "\n",
      "\n",
      "Text: its a totally average film with a few semi-alright action sequences that make the plot seem a little better and remind the viewer of the classic van dam films. parts of the plot don't make sense and seem to be added in to use up time. the end plot is that of a very basic type that doesn't leave the viewer guessing and any twists are obvious from the beginning. the end scene with the flask backs don't make sense as they are added in and seem to have little relevance to the history of van dam's character. not really worth watching again, bit disappointed in the end production, even though it is apparent it was shot on a low budget certain shots and sections in the film are of poor directed quality\n",
      "\n",
      "\n",
      "Prediction: NEGATIVE, Confidence: 0.9997100234031677\n",
      "\n",
      "\n",
      "Text: STAR RATING: ***** Saturday Night **** Friday Night *** Friday Morning ** Sunday Night * Monday Morning <br /><br />Former New Orleans homicide cop Jack Robideaux (Jean Claude Van Damme) is re-assigned to Columbus, a small but violent town in Mexico to help the police there with their efforts to stop a major heroin smuggling operation into their town. The culprits turn out to be ex-military, lead by former commander Benjamin Meyers (Stephen Lord, otherwise known as Jase from East Enders) who is using a special method he learned in Afghanistan to fight off his opponents. But Jack has a more personal reason for taking him down, that draws the two men into an explosive final showdown where only one will walk away alive.<br /><br />After Until Death, Van Damme appeared to be on a high, showing he could make the best straight to video films in the action market. While that was a far more drama oriented film, with The Shepherd he has returned to the high-kicking, no brainer action that first made him famous and has sadly produced his worst film since Derailed. It's nowhere near as bad as that film, but what I said still stands.<br /><br />A dull, predictable film, with very little in the way of any exciting action. What little there is mainly consists of some limp fight scenes, trying to look cool and trendy with some cheap slo-mo/sped up effects added to them that sadly instead make them look more desperate. Being a Mexican set film, director Isaac Florentine has tried to give the film a Robert Rodriguez/Desperado sort of feel, but this only adds to the desperation.<br /><br />VD gives a particularly uninspired performance and given he's never been a Robert De Niro sort of actor, that can't be good. As the villain, Lord shouldn't expect to leave the beeb anytime soon. He gets little dialogue at the beginning as he struggles to muster an American accent but gets mysteriously better towards the end. All the supporting cast are equally bland, and do nothing to raise the films spirits at all.<br /><br />This is one shepherd that's strayed right from the flock. *\n",
      "\n",
      "\n",
      "Prediction: NEGATIVE, Confidence: 0.995756208896637\n",
      "\n",
      "\n",
      "Text: First off let me say, If you haven't enjoyed a Van Damme movie since bloodsport, you probably will not like this movie. Most of these movies may not have the best plots or best actors but I enjoy these kinds of movies for what they are. This movie is much better than any of the movies the other action guys (Segal and Dolph) have thought about putting out the past few years. Van Damme is good in the movie, the movie is only worth watching to Van Damme fans. It is not as good as Wake of Death (which i highly recommend to anyone of likes Van Damme) or In hell but, in my opinion it's worth watching. It has the same type of feel to it as Nowhere to Run. Good fun stuff!\n",
      "\n",
      "\n",
      "Prediction: POSITIVE, Confidence: 0.996307373046875\n",
      "\n",
      "\n",
      "Text: I had high hopes for this one until they changed the name to 'The Shepherd : Border Patrol, the lamest movie name ever, what was wrong with just 'The Shepherd'. This is a by the numbers action flick that tips its hat at many classic Van Damme films. There is a nice bit of action in a bar which reminded me of hard target and universal soldier but directed with no intensity or flair which is a shame. There is one great line about 'being p*ss drunk and carrying a rabbit' and some OK action scenes let down by the cheapness of it all. A lot of the times the dialogue doesn't match the characters mouth and the stunt men fall down dead a split second before even being shot. The end fight is one of the better Van Damme fights except the Director tries to go a bit too John Woo and fails also introducing flashbacks which no one really cares about just gets in the way of the action which is the whole point of a van Damme film.<br /><br />Not good, not bad, just average generic action.\n",
      "\n",
      "\n",
      "Prediction: NEGATIVE, Confidence: 0.9966711401939392\n",
      "\n",
      "\n",
      "Text: Isaac Florentine has made some of the best western Martial Arts action movies ever produced. In particular US Seals 2, Cold Harvest, Special Forces and Undisputed 2 are all action classics. You can tell Isaac has a real passion for the genre and his films are always eventful, creative and sharp affairs, with some of the best fight sequences an action fan could hope for. In particular he has found a muse with Scott Adkins, as talented an actor and action performer as you could hope for. This is borne out with Special Forces and Undisputed 2, but unfortunately The Shepherd just doesn't live up to their abilities.<br /><br />There is no doubt that JCVD looks better here fight-wise than he has done in years, especially in the fight he has (for pretty much no reason) in a prison cell, and in the final showdown with Scott, but look in his eyes. JCVD seems to be dead inside. There's nothing in his eyes at all. It's like he just doesn't care about anything throughout the whole film. And this is the leading man.<br /><br />There are other dodgy aspects to the film, script-wise and visually, but the main problem is that you are utterly unable to empathise with the hero of the film. A genuine shame as I know we all wanted this film to be as special as it genuinely could have been. There are some good bits, mostly the action scenes themselves. This film had a terrific director and action choreographer, and an awesome opponent for JCVD to face down. This could have been the one to bring the veteran action star back up to scratch in the balls-out action movie stakes.<br /><br />Sincerely a shame that this didn't happen.\n",
      "\n",
      "\n",
      "Prediction: NEGATIVE, Confidence: 0.9584169983863831\n",
      "\n",
      "\n",
      "Text: It actually pains me to say it, but this movie was horrible on every level. The blame does not lie entirely with Van Damme as you can see he tried his best, but let's face it, he's almost fifty, how much more can you ask of him? I find it so hard to believe that the same people who put together Undisputed 2; arguably the best (western) martial arts movie in years, created this. Everything from the plot, to the dialog, to the editing, to the overall acting was just horribly put together and in many cases outright boring and nonsensical. Scott Adkins who's fight scenes seemed more like a demo reel, was also terribly underused and not even the main villain which is such a shame because 1) He is more than capable of playing that role and 2) The actual main villain was not only not intimidating at all but also quite annoying. Again, not blaming Van Damme. I will always be a fan, but avoid this one.\n",
      "\n",
      "\n",
      "Prediction: NEGATIVE, Confidence: 0.9994093179702759\n",
      "\n",
      "\n",
      "Text: Technically I'am a Van Damme Fan, or I was. this movie is so bad that I hated myself for wasting those 90 minutes. Do not let the name Isaac Florentine (Undisputed II) fool you, I had big hopes for this one, depending on what I saw in (Undisputed II), man.. was I wrong ??! all action fans wanted a big comeback for the classic action hero, but i guess we wont be able to see that soon, as our hero keep coming with those (going -to-a-border - far-away-town-and -kill -the-bad-guys- than-comeback- home) movies I mean for God's sake, we are in 2008, and they insist on doing those disappointing movies on every level. Why ??!!! Do your self a favor, skip it.. seriously.\n",
      "\n",
      "\n",
      "Prediction: NEGATIVE, Confidence: 0.9996923208236694\n",
      "\n",
      "\n",
      "Text: Honestly awful film, bad editing, awful lighting, dire dialog and scrappy screenplay.<br /><br />The lighting at is so bad there's moments you can't even see what's going on, I even tried to playing with the contrast and brightness so I could see something but that didn't help.<br /><br />They must have found the script in a bin, the character development is just as awful and while you hardly expect much from a Jean-Claude Van Damme film this one manages to hit an all time low. You can't even laugh at the cheesy'ness.<br /><br />The directing and editing are also terrible, the whole film follows an extremely tired routine and fails at every turn as it bumbles through the plot that is so weak it's just unreal.<br /><br />There's not a lot else to say other than it's really bad and nothing like Jean-Claude Van Damme's earlier work which you could enjoy.<br /><br />Avoid like the plaque, frankly words fail me in condemning this \"film\".\n",
      "\n",
      "\n",
      "Prediction: NEGATIVE, Confidence: 0.99977046251297\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----- line break -----\n",
      "\n",
      "\n",
      "\n",
      "RoBERTa Raw Output (first 10 examples):\n",
      "Text: I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn't match the background, and painfully one-dimensional characters cannot be overcome with a 'sci-fi' setting. (I'm sure there are those of you out there who think Babylon 5 is good sci-fi TV. It's not. It's clichéd and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It's really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it's rubbish as they have to always say \"Gene Roddenberry's Earth...\" otherwise people would not continue watching. Roddenberry's ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.\n",
      "\n",
      "\n",
      "Prediction: negative, Confidence: 0.4153352677822113\n",
      "\n",
      "\n",
      "Text: Worth the entertainment value of a rental, especially if you like action movies. This one features the usual car chases, fights with the great Van Damme kick style, shooting battles with the 40 shell load shotgun, and even terrorist style bombs. All of this is entertaining and competently handled but there is nothing that really blows you away if you've seen your share before.<br /><br />The plot is made interesting by the inclusion of a rabbit, which is clever but hardly profound. Many of the characters are heavily stereotyped -- the angry veterans, the terrified illegal aliens, the crooked cops, the indifferent feds, the bitchy tough lady station head, the crooked politician, the fat federale who looks like he was typecast as the Mexican in a Hollywood movie from the 1940s. All passably acted but again nothing special.<br /><br />I thought the main villains were pretty well done and fairly well acted. By the end of the movie you certainly knew who the good guys were and weren't. There was an emotional lift as the really bad ones got their just deserts. Very simplistic, but then you weren't expecting Hamlet, right? The only thing I found really annoying was the constant cuts to VDs daughter during the last fight scene.<br /><br />Not bad. Not good. Passable 4.\n",
      "\n",
      "\n",
      "Prediction: positive, Confidence: 0.6511966586112976\n",
      "\n",
      "\n",
      "Text: its a totally average film with a few semi-alright action sequences that make the plot seem a little better and remind the viewer of the classic van dam films. parts of the plot don't make sense and seem to be added in to use up time. the end plot is that of a very basic type that doesn't leave the viewer guessing and any twists are obvious from the beginning. the end scene with the flask backs don't make sense as they are added in and seem to have little relevance to the history of van dam's character. not really worth watching again, bit disappointed in the end production, even though it is apparent it was shot on a low budget certain shots and sections in the film are of poor directed quality\n",
      "\n",
      "\n",
      "Prediction: negative, Confidence: 0.8418298363685608\n",
      "\n",
      "\n",
      "Text: STAR RATING: ***** Saturday Night **** Friday Night *** Friday Morning ** Sunday Night * Monday Morning <br /><br />Former New Orleans homicide cop Jack Robideaux (Jean Claude Van Damme) is re-assigned to Columbus, a small but violent town in Mexico to help the police there with their efforts to stop a major heroin smuggling operation into their town. The culprits turn out to be ex-military, lead by former commander Benjamin Meyers (Stephen Lord, otherwise known as Jase from East Enders) who is using a special method he learned in Afghanistan to fight off his opponents. But Jack has a more personal reason for taking him down, that draws the two men into an explosive final showdown where only one will walk away alive.<br /><br />After Until Death, Van Damme appeared to be on a high, showing he could make the best straight to video films in the action market. While that was a far more drama oriented film, with The Shepherd he has returned to the high-kicking, no brainer action that first made him famous and has sadly produced his worst film since Derailed. It's nowhere near as bad as that film, but what I said still stands.<br /><br />A dull, predictable film, with very little in the way of any exciting action. What little there is mainly consists of some limp fight scenes, trying to look cool and trendy with some cheap slo-mo/sped up effects added to them that sadly instead make them look more desperate. Being a Mexican set film, director Isaac Florentine has tried to give the film a Robert Rodriguez/Desperado sort of feel, but this only adds to the desperation.<br /><br />VD gives a particularly uninspired performance and given he's never been a Robert De Niro sort of actor, that can't be good. As the villain, Lord shouldn't expect to leave the beeb anytime soon. He gets little dialogue at the beginning as he struggles to muster an American accent but gets mysteriously better towards the end. All the supporting cast are equally bland, and do nothing to raise the films spirits at all.<br /><br />This is one shepherd that's strayed right from the flock. *\n",
      "\n",
      "\n",
      "Prediction: neutral, Confidence: 0.8516733646392822\n",
      "\n",
      "\n",
      "Text: First off let me say, If you haven't enjoyed a Van Damme movie since bloodsport, you probably will not like this movie. Most of these movies may not have the best plots or best actors but I enjoy these kinds of movies for what they are. This movie is much better than any of the movies the other action guys (Segal and Dolph) have thought about putting out the past few years. Van Damme is good in the movie, the movie is only worth watching to Van Damme fans. It is not as good as Wake of Death (which i highly recommend to anyone of likes Van Damme) or In hell but, in my opinion it's worth watching. It has the same type of feel to it as Nowhere to Run. Good fun stuff!\n",
      "\n",
      "\n",
      "Prediction: positive, Confidence: 0.8244377374649048\n",
      "\n",
      "\n",
      "Text: I had high hopes for this one until they changed the name to 'The Shepherd : Border Patrol, the lamest movie name ever, what was wrong with just 'The Shepherd'. This is a by the numbers action flick that tips its hat at many classic Van Damme films. There is a nice bit of action in a bar which reminded me of hard target and universal soldier but directed with no intensity or flair which is a shame. There is one great line about 'being p*ss drunk and carrying a rabbit' and some OK action scenes let down by the cheapness of it all. A lot of the times the dialogue doesn't match the characters mouth and the stunt men fall down dead a split second before even being shot. The end fight is one of the better Van Damme fights except the Director tries to go a bit too John Woo and fails also introducing flashbacks which no one really cares about just gets in the way of the action which is the whole point of a van Damme film.<br /><br />Not good, not bad, just average generic action.\n",
      "\n",
      "\n",
      "Prediction: negative, Confidence: 0.7441726326942444\n",
      "\n",
      "\n",
      "Text: Isaac Florentine has made some of the best western Martial Arts action movies ever produced. In particular US Seals 2, Cold Harvest, Special Forces and Undisputed 2 are all action classics. You can tell Isaac has a real passion for the genre and his films are always eventful, creative and sharp affairs, with some of the best fight sequences an action fan could hope for. In particular he has found a muse with Scott Adkins, as talented an actor and action performer as you could hope for. This is borne out with Special Forces and Undisputed 2, but unfortunately The Shepherd just doesn't live up to their abilities.<br /><br />There is no doubt that JCVD looks better here fight-wise than he has done in years, especially in the fight he has (for pretty much no reason) in a prison cell, and in the final showdown with Scott, but look in his eyes. JCVD seems to be dead inside. There's nothing in his eyes at all. It's like he just doesn't care about anything throughout the whole film. And this is the leading man.<br /><br />There are other dodgy aspects to the film, script-wise and visually, but the main problem is that you are utterly unable to empathise with the hero of the film. A genuine shame as I know we all wanted this film to be as special as it genuinely could have been. There are some good bits, mostly the action scenes themselves. This film had a terrific director and action choreographer, and an awesome opponent for JCVD to face down. This could have been the one to bring the veteran action star back up to scratch in the balls-out action movie stakes.<br /><br />Sincerely a shame that this didn't happen.\n",
      "\n",
      "\n",
      "Prediction: positive, Confidence: 0.8726266622543335\n",
      "\n",
      "\n",
      "Text: It actually pains me to say it, but this movie was horrible on every level. The blame does not lie entirely with Van Damme as you can see he tried his best, but let's face it, he's almost fifty, how much more can you ask of him? I find it so hard to believe that the same people who put together Undisputed 2; arguably the best (western) martial arts movie in years, created this. Everything from the plot, to the dialog, to the editing, to the overall acting was just horribly put together and in many cases outright boring and nonsensical. Scott Adkins who's fight scenes seemed more like a demo reel, was also terribly underused and not even the main villain which is such a shame because 1) He is more than capable of playing that role and 2) The actual main villain was not only not intimidating at all but also quite annoying. Again, not blaming Van Damme. I will always be a fan, but avoid this one.\n",
      "\n",
      "\n",
      "Prediction: negative, Confidence: 0.8934266567230225\n",
      "\n",
      "\n",
      "Text: Technically I'am a Van Damme Fan, or I was. this movie is so bad that I hated myself for wasting those 90 minutes. Do not let the name Isaac Florentine (Undisputed II) fool you, I had big hopes for this one, depending on what I saw in (Undisputed II), man.. was I wrong ??! all action fans wanted a big comeback for the classic action hero, but i guess we wont be able to see that soon, as our hero keep coming with those (going -to-a-border - far-away-town-and -kill -the-bad-guys- than-comeback- home) movies I mean for God's sake, we are in 2008, and they insist on doing those disappointing movies on every level. Why ??!!! Do your self a favor, skip it.. seriously.\n",
      "\n",
      "\n",
      "Prediction: negative, Confidence: 0.864944338798523\n",
      "\n",
      "\n",
      "Text: Honestly awful film, bad editing, awful lighting, dire dialog and scrappy screenplay.<br /><br />The lighting at is so bad there's moments you can't even see what's going on, I even tried to playing with the contrast and brightness so I could see something but that didn't help.<br /><br />They must have found the script in a bin, the character development is just as awful and while you hardly expect much from a Jean-Claude Van Damme film this one manages to hit an all time low. You can't even laugh at the cheesy'ness.<br /><br />The directing and editing are also terrible, the whole film follows an extremely tired routine and fails at every turn as it bumbles through the plot that is so weak it's just unreal.<br /><br />There's not a lot else to say other than it's really bad and nothing like Jean-Claude Van Damme's earlier work which you could enjoy.<br /><br />Avoid like the plaque, frankly words fail me in condemning this \"film\".\n",
      "\n",
      "\n",
      "Prediction: negative, Confidence: 0.9320728182792664\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# raw predictions from DistilBERT on the first 10 IMDB reviews\n",
    "distilbert_raw_output = model1(imdb_texts[:10])\n",
    "\n",
    "# raw output to inspect the probabilities\n",
    "print(\"DistilBERT Raw Output (first 10 examples):\")\n",
    "for i, output in enumerate(distilbert_raw_output):\n",
    "    print(f\"Text: {imdb_texts[i]}\")\n",
    "    print('\\n')\n",
    "    print(f\"Prediction: {output['label']}, Confidence: {output['score']}\")\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "print ('----- line break -----')\n",
    "print('\\n')\n",
    "\n",
    "# raw predictions from RoBERTa on the first 10 IMDB reviews\n",
    "roberta_raw_output = model2(imdb_texts[:10])\n",
    "\n",
    "# raw output to inspect the probabilities\n",
    "print(\"\\nRoBERTa Raw Output (first 10 examples):\")\n",
    "for i, output in enumerate(roberta_raw_output):\n",
    "    print(f\"Text: {imdb_texts[i]}\")\n",
    "    print('\\n')\n",
    "    print(f\"Prediction: {output['label']}, Confidence: {output['score']}\")\n",
    "    print('\\n')\n",
    "    \n",
    "# the point of this is to check that these models are: \n",
    "# 1) actually outputting something\n",
    "# 2) if the output is upper/lower case\n",
    "# 3) how much confidence there is (potentially set a confidence threshold in the future thats acceptable)\n",
    "\n",
    "# we see that these raw outputs clearly exist so there must be some problem with function logic or \n",
    "# something potentially wrong with the model being trained for identifying positive instances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (x, y) in enumerate(zip([1,2,3], ['a','b','c'])):\n",
    "    # print(f\"Index {i}: {x} -> {y}\")\n",
    "\n",
    "# Index 0: 1 -> a\n",
    "# Index 1: 2 -> b\n",
    "# Index 2: 3 -> c\n",
    "\n",
    "# demonstration of enumerate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 855980.41it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# maybe the model is not seeing \"POSITIVE\" and \"positive\" so hard code it to make sure it's .lower() and .upper()\n",
    "# run prediction\n",
    "\n",
    "imdb_prediction1 = [\n",
    "    1 if r[\"label\"].lower() == \"positive\" \n",
    "    else 0 if r[\"label\"].lower() == \"negative\" \n",
    "    else None for r in tqdm(model1(imdb_texts, batch_size=32, truncation=True, padding=True))\n",
    "]\n",
    "\n",
    "yelp_prediction1 = [\n",
    "    1 if r[\"label\"].lower() == \"positive\" \n",
    "    else 0 if r[\"label\"].lower() == \"negative\" \n",
    "    else None for r in tqdm(model1(yelp_texts, batch_size=32, truncation=True, padding=True))\n",
    "]\n",
    "\n",
    "imdb_prediction2 = [1 if r[\"label\"].lower() == \"positive\" else 0 for r in model2(imdb_texts)]\n",
    "yelp_prediction2 = [1 if r[\"label\"].lower() == \"positive\" else 0 for r in model2(yelp_texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBERT IMDB Accuracy: 0.908, F1: 0.0\n",
      "DistilBERT Yelp Accuracy: 0.891, F1: 0.883919062832801\n",
      "RoBERTa IMDB Accuracy: 0.888, F1: 0.0\n",
      "RoBERTa Yelp Accuracy: 0.858, F1: 0.8582834331337326\n"
     ]
    }
   ],
   "source": [
    "# copy paste code from above and try again to calculate performance \n",
    "def calculate_performance(predictions, true_labels):\n",
    "    # filter out None values (for neutral predictions) and align with actual labels\n",
    "    valid_predictions = [p for p in predictions if p is not None]\n",
    "    valid_labels = [true_labels[i] for i in range(len(predictions)) if predictions[i] is not None]\n",
    "    \n",
    "    accuracy = accuracy_score(valid_labels, valid_predictions)\n",
    "    f1 = f1_score(valid_labels, valid_predictions, zero_division=1)  # avoid undefined metric errors\n",
    "    return accuracy, f1\n",
    "\n",
    "distilbert_imdb_accuracy, distilbert_imdb_f1 = calculate_performance(imdb_prediction1, imdb_labels)\n",
    "distilbert_yelp_accuracy, distilbert_yelp_f1 = calculate_performance(yelp_prediction1, yelp_labels)\n",
    "roberta_imdb_accuracy, roberta_imdb_f1 = calculate_performance(imdb_prediction2, imdb_labels)\n",
    "roberta_yelp_accuracy, roberta_yelp_f1 = calculate_performance(yelp_prediction2, yelp_labels)\n",
    "\n",
    "print(f\"DistilBERT IMDB Accuracy: {distilbert_imdb_accuracy}, F1: {distilbert_imdb_f1}\")\n",
    "print(f\"DistilBERT Yelp Accuracy: {distilbert_yelp_accuracy}, F1: {distilbert_yelp_f1}\")\n",
    "print(f\"RoBERTa IMDB Accuracy: {roberta_imdb_accuracy}, F1: {roberta_imdb_f1}\")\n",
    "print(f\"RoBERTa Yelp Accuracy: {roberta_yelp_accuracy}, F1: {roberta_yelp_f1}\")\n",
    "\n",
    "# f1 still producing 0, so it can't be that the model is not recognizing the lower/upper case classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBERT Classification Report (IMDB):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95      1000\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.91      1000\n",
      "   macro avg       0.50      0.45      0.48      1000\n",
      "weighted avg       1.00      0.91      0.95      1000\n",
      "\n",
      "DistilBERT Classification Report (Yelp):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.92      0.90       518\n",
      "           1       0.91      0.86      0.88       482\n",
      "\n",
      "    accuracy                           0.89      1000\n",
      "   macro avg       0.89      0.89      0.89      1000\n",
      "weighted avg       0.89      0.89      0.89      1000\n",
      "\n",
      "RoBERTa Classification Report (IMDB):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.89      0.94      1000\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.89      1000\n",
      "   macro avg       0.50      0.44      0.47      1000\n",
      "weighted avg       1.00      0.89      0.94      1000\n",
      "\n",
      "RoBERTa Classification Report (Yelp):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.83      0.86       518\n",
      "           1       0.83      0.89      0.86       482\n",
      "\n",
      "    accuracy                           0.86      1000\n",
      "   macro avg       0.86      0.86      0.86      1000\n",
      "weighted avg       0.86      0.86      0.86      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raiu\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\raiu\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\raiu\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\raiu\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\raiu\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\raiu\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(\"DistilBERT Classification Report (IMDB):\")\n",
    "print(classification_report(imdb_labels, imdb_prediction1))\n",
    "\n",
    "print(\"DistilBERT Classification Report (Yelp):\")\n",
    "print(classification_report(yelp_labels, yelp_prediction1))\n",
    "\n",
    "print(\"RoBERTa Classification Report (IMDB):\")\n",
    "print(classification_report(imdb_labels, imdb_prediction2))\n",
    "\n",
    "print(\"RoBERTa Classification Report (Yelp):\")\n",
    "print(classification_report(yelp_labels, yelp_prediction2))\n",
    "\n",
    "# for IMDB: \n",
    "# the classification report is showing that \"1\" is not being detected and that f1 is defaulting to 0.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error message:\n",
    "C:\\Users\\raiu\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
    "  _warn_prf(average, modifier, msg_start, len(result))\n",
    "C:\\Users\\raiu\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
    "  _warn_prf(average, modifier, msg_start, len(result))\n",
    "C:\\Users\\raiu\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
    "  _warn_prf(average, modifier, msg_start, len(result))\n",
    "C:\\Users\\raiu\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
    "  _warn_prf(average, modifier, msg_start, len(result))\n",
    "C:\\Users\\raiu\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
    "  _warn_prf(average, modifier, msg_start, len(result))\n",
    "C:\\Users\\raiu\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
    "  _warn_prf(average, modifier, msg_start, len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB Size: 1000\n",
      "Yelp Size: 1000\n"
     ]
    }
   ],
   "source": [
    "# we can try balancing the dataset because the model might be overfitting? error says \"zero division\" as in\n",
    "# 0 divided by soemthing?\n",
    "\n",
    "# balance dataset for 500 positive + negative instances (1000 samples from each class)\n",
    "imdb_pos = [x for x, y in zip(imdb[\"test\"][\"text\"], imdb[\"test\"][\"label\"]) if y == 1][:500]\n",
    "imdb_neg = [x for x, y in zip(imdb[\"test\"][\"text\"], imdb[\"test\"][\"label\"]) if y == 0][:500]\n",
    "imdb_texts_balanced = imdb_pos + imdb_neg\n",
    "imdb_labels_balanced = [1] * 500 + [0] * 500\n",
    "\n",
    "yelp_pos = [x for x, y in zip(yelp[\"test\"][\"text\"], yelp[\"test\"][\"label\"]) if y == 1][:500]\n",
    "yelp_neg = [x for x, y in zip(yelp[\"test\"][\"text\"], yelp[\"test\"][\"label\"]) if y == 0][:500]\n",
    "yelp_texts_balanced = yelp_pos + yelp_neg\n",
    "yelp_labels_balanced = [1] * 500 + [0] * 500\n",
    "\n",
    "# confirm that the final sizes after balance are 1000 still\n",
    "print(f\"IMDB Size: {len(imdb_texts_balanced)}\")\n",
    "print(f\"Yelp Size: {len(yelp_texts_balanced)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper to get predictions\n",
    "def get_predictions(model, texts):\n",
    "    preds = model(texts, batch_size=32, truncation=True, padding=True, max_length=512)\n",
    "    mapped = []\n",
    "    for r in preds:\n",
    "        label = r[\"label\"].lower()\n",
    "        if label == \"positive\":\n",
    "            mapped.append(1)\n",
    "        elif label == \"negative\":\n",
    "            mapped.append(0)\n",
    "        else:\n",
    "            mapped.append(None)  # neutral or unknown\n",
    "    return mapped\n",
    "\n",
    "# get predictions for DistilBERT\n",
    "imdb_prediction1 = get_predictions(model1, imdb_texts_balanced)\n",
    "yelp_prediction1 = get_predictions(model1, yelp_texts_balanced)\n",
    "\n",
    "# predictions for RoBERTa\n",
    "imdb_prediction2 = get_predictions(model2, imdb_texts_balanced)\n",
    "yelp_prediction2 = get_predictions(model2, yelp_texts_balanced)\n",
    "\n",
    "# \"You seem to be using the pipelines sequentially on GPU. \n",
    "# In order to maximize efficiency please use a dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DistilBERT on IMDB | Accuracy: 0.881 | F1: 0.878\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.863     0.906     0.884       500\n",
      "           1      0.901     0.856     0.878       500\n",
      "\n",
      "    accuracy                          0.881      1000\n",
      "   macro avg      0.882     0.881     0.881      1000\n",
      "weighted avg      0.882     0.881     0.881      1000\n",
      "\n",
      "\n",
      "DistilBERT on Yelp | Accuracy: 0.891 | F1: 0.888\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.870     0.920     0.894       500\n",
      "           1      0.915     0.862     0.888       500\n",
      "\n",
      "    accuracy                          0.891      1000\n",
      "   macro avg      0.892     0.891     0.891      1000\n",
      "weighted avg      0.892     0.891     0.891      1000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RoBERTa on IMDB | Accuracy: 0.874 | F1: 0.872\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.898     0.853     0.875       402\n",
      "           1      0.850     0.896     0.872       374\n",
      "\n",
      "    accuracy                          0.874       776\n",
      "   macro avg      0.874     0.874     0.874       776\n",
      "weighted avg      0.875     0.874     0.874       776\n",
      "\n",
      "\n",
      "RoBERTa on Yelp | Accuracy: 0.885 | F1: 0.895\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.951     0.804     0.872       439\n",
      "           1      0.838     0.961     0.895       462\n",
      "\n",
      "    accuracy                          0.885       901\n",
      "   macro avg      0.895     0.883     0.883       901\n",
      "weighted avg      0.893     0.885     0.884       901\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8845726970033296, 0.8951612903225807)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluation function\n",
    "def calculate_performance(predictions, labels, model_name, dataset_name):    \n",
    "    valid_predictions = [p for p in predictions if p is not None]\n",
    "    valid_labels = [labels[i] for i in range(len(predictions)) if predictions[i] is not None]\n",
    "\n",
    "    accuracy = accuracy_score(valid_labels, valid_predictions)\n",
    "    f1 = f1_score(valid_labels, valid_predictions, zero_division=1)\n",
    "    \n",
    "    print(f\"\\n{model_name} on {dataset_name} | Accuracy: {accuracy:.3f} | F1: {f1:.3f}\")\n",
    "    print(classification_report(valid_labels, valid_predictions, digits=3))\n",
    "    \n",
    "    return accuracy, f1\n",
    "\n",
    "# find results from matrix\n",
    "calculate_performance(imdb_prediction1, imdb_labels_balanced, \"DistilBERT\", \"IMDB\")\n",
    "calculate_performance(yelp_prediction1, yelp_labels_balanced, \"DistilBERT\", \"Yelp\")\n",
    "print('\\n')\n",
    "calculate_performance(imdb_prediction2, imdb_labels_balanced, \"RoBERTa\", \"IMDB\")\n",
    "calculate_performance(yelp_prediction2, yelp_labels_balanced, \"RoBERTa\", \"Yelp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# originally: \n",
    "\n",
    "# DistilBERT IMDB Accuracy: 0.908, F1: 0.0\n",
    "# DistilBERT Yelp Accuracy: 0.891, F1: 0.883919062832801\n",
    "# RoBERTa IMDB Accuracy: 0.888, F1: 0.0\n",
    "# RoBERTa Yelp Accuracy: 0.858, F1: 0.8582834331337326\n",
    "\n",
    "# previously, there was no positive sample present, f1 is calculated on how well you predict both 0 and 1\n",
    "# if we balance the dataset to support 500 positive and 500 negative examples, then the classifier is\n",
    "# asked to classify both classes during eval, so precision, recall, and f1 are all computable \n",
    "\n",
    "# f1 = 2 (precision)*(recall)/(precision + recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0rW2voE8vJ5G"
   },
   "source": [
    "## 4. Compare Models & Do a Short Error Analysis\n",
    "After running inference on your test set:\n",
    "- Compare Metrics: Which model is most accurate overall? Does one have higher F1?\n",
    "- Identify Edge Cases:\n",
    "\n",
    "  - Look at ~5 examples that were misclassified by at least one model.\n",
    "  - What patterns do you see? (e.g., tricky wording, sarcasm, short text, etc.) If you don't see any pattern that's fine but make sure you've looked hard enough (maybe you need more than 5 examples?)\n",
    "  - What examples do all models make mistakes on? What mistakes are unique to a particular model? (again, if you dont find a patter that fine but make sure you've tried a lot of things)\n",
    "\n",
    "####  Write a short paragraph or make a small table summarizing your findings:\n",
    "\n",
    "- Which model performed best overall?\n",
    "- Any surprising differences?\n",
    "- How might you improve performance further?\n",
    "\n",
    "#### Deliverables\n",
    "- Code: A Python script or Jupyter notebook showing how you:\n",
    "  - Load data (and potentially preprocess it).\n",
    "  - Instantiate the Hugging Face pipelines.\n",
    "  - Run predictions and calculate metrics.\n",
    "\n",
    "- Short text on findings  (in a text box in your Colab notebook):\n",
    "  - Which dataset(s) you chose and why.\n",
    "  - The models you compared and a table/plot of accuracy or F1 (use matplotlib for graphs).\n",
    "  - Example misclassified cases and your hypothesis for why they failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate metrics\n",
    "metrics = {}\n",
    "\n",
    "metrics[\"DistilBERT-IMDB\"] = calculate_performance(imdb_prediction1, imdb_labels_balanced, \"DistilBERT\", \"IMDB\")\n",
    "metrics[\"DistilBERT-Yelp\"] = calculate_performance(yelp_prediction1, yelp_labels_balanced, \"DistilBERT\", \"Yelp\")\n",
    "metrics[\"RoBERTa-IMDB\"]   = calculate_performance(imdb_prediction2, imdb_labels_balanced, \"RoBERTa\", \"IMDB\")\n",
    "metrics[\"RoBERTa-Yelp\"]   = calculate_performance(yelp_prediction2, yelp_labels_balanced, \"RoBERTa\", \"Yelp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t7j8FPYj30B2"
   },
   "outputs": [],
   "source": [
    "# auto-updating Plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels_plot = list(metrics.keys())\n",
    "accuracies = [metrics[k][0] for k in labels_plot]\n",
    "f1_scores  = [metrics[k][1] for k in labels_plot]\n",
    "\n",
    "x = range(len(labels_plot))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar([i - width/2 for i in x], accuracies, width=width, label=\"Accuracy\")\n",
    "plt.bar([i + width/2 for i in x], f1_scores, width=width, label=\"F1 Score\")\n",
    "\n",
    "for i, (a, f) in enumerate(zip(accuracies, f1_scores)):\n",
    "    plt.text(i - width/2, a + 0.01, f\"{a:.3f}\", ha='center')\n",
    "    plt.text(i + width/2, f + 0.01, f\"{f:.3f}\", ha='center')\n",
    "    \n",
    "plt.xticks(x, labels_plot, rotation=30)\n",
    "plt.ylabel(\"Scores\")\n",
    "plt.title(\"Model Performance\")\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.25), ncol=2, frameon=False) # move legend to middle of plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 is = 2(Precision * Recall)/(Precision + Recall). Accuracy = correct predictions/total predictions and this is fine when the classes are balanced (in my case, I had to balance the positive and negatives), and false positives + false negatives have to be equal cost too. Accuracy can be misleading because if the dataset is imbalanced with 90% negative and 10% positive, you care more about either catching all positives (Recall) or making sure your positives are really positive (Precision). We use F1 because sentiment data is oftentimes imbalanced, but I picked a small enough dataset and balanced one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One pattern I see that the model is not the best at classifying is when we see somebody write a \"not bad\"-esque review where they say something along the lines of \"bang for buck, but not the greatest movie in the world\". The DistilBERT model classifies this as negative. For example: \"Text: Worth the entertainment value of a rental, especially if you like action movies [...] The only thing I found really annoying was the constant cuts to VDs daughter during the last fight scene.<br /><br />Not bad. Not good. Passable 4. Prediction: NEGATIVE, Confidence: 0.6170620322227478\". In this example text, RoBERTa actually classified it as \"Prediction: positive, Confidence: 0.6511966586112976\"! \n",
    "\n",
    "This is a model-specific mistake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think it's naive to say that that model with the highest F1 score is the best performing. There's nuance with evaluation. Cases where the classes are balanced make it easier to default to F1, but there are cases where that would still not tell the full story. \n",
    "\n",
    "But in our case, since both RoBERTa and DistilBERT are operating under the same circumstances with balanced dataset, we can compare how both perform under the same two datasets. \n",
    "\n",
    "RoBERTa handles positive/balance better on the Yelp dataset, likely avoiding bias. RoBERTa's yelp F1 score is higher than accuracy which means that the model is better at handling imbalanced between precision and recall (even if the total count of correct classifications is lower), so it's less likely to make major errors on Yelp dataset, even if it does have slightly more numbers of incorrect classifications entirely. \n",
    "\n",
    "Both models do better on Yelp than IMDB. This is probably because Yelp has less sarcasm, humor, speech patterns, etc. and so probably because IMDB probably has lengthier discussions about the film. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sentiment tasks, we should use F1 to judge performance, and RoBERTa was highest F1. RoBERTa is a bigger model so you expect it to perform more across the board, but DistilBERT actually performed better at IMDB for some cases. This means that IMDB reviews may be more similar to the dataset that DilstilBERT was trained on, whereas RoBERTa might not be able to transition from Twitter-text to long movie reviews easily. \n",
    "\n",
    "It's also a little strange that F1 score ended up being higher than accuracy in one instance, but it's possible when the model balances precision and recall well. RoBERTa might be more consistently positive-leaning like \"glass-half full\"-type model, whereas DistilBERT might be more like \"glass-half empty\". "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNQNVhBEJS5KeGyY7XGbDuz",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "22f23175f9f44cdda4b6229f936bb1cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "252b1bb4b7c3484fac23ab875e4f6755": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "36759acb17d542d7919eb4d77cf45b6e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "48779259ce54415da1ee921ff98dfa25": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "54273996fb2640c3b54bc976e3d4a776": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_48779259ce54415da1ee921ff98dfa25",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b301ca104f1245fca0bd09c9df565a78",
      "value": 2
     }
    },
    "8d5daf6318d6481089733d457d390466": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c53a7183dc14a49b65b48380b16860a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aaedd6c01c634250ad6aeed3e45d19e7",
      "placeholder": "​",
      "style": "IPY_MODEL_22f23175f9f44cdda4b6229f936bb1cd",
      "value": " 2/2 [01:15&lt;00:00, 35.24s/it]"
     }
    },
    "9d717440b4bd49738d91b7d51b8b1758": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fd0497ece5c0470bb5c3159f996e79a4",
       "IPY_MODEL_54273996fb2640c3b54bc976e3d4a776",
       "IPY_MODEL_9c53a7183dc14a49b65b48380b16860a"
      ],
      "layout": "IPY_MODEL_36759acb17d542d7919eb4d77cf45b6e"
     }
    },
    "aaedd6c01c634250ad6aeed3e45d19e7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b301ca104f1245fca0bd09c9df565a78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fd0497ece5c0470bb5c3159f996e79a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8d5daf6318d6481089733d457d390466",
      "placeholder": "​",
      "style": "IPY_MODEL_252b1bb4b7c3484fac23ab875e4f6755",
      "value": "Loading checkpoint shards: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
