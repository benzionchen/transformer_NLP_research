{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/benzionchen/transformer_NLP_research/blob/main/transformer_research_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wp1Z6WIbjyIN"
   },
   "source": [
    "The goal is for you to practice:\n",
    "\n",
    "- Picking a dataset (and possibly trying more than one).\n",
    "- Selecting different pretrained models from the Hugging Face Hub.\n",
    "- Measuring performance (accuracy, F1, etc.) and comparing your results.\n",
    "- Doing a brief error analysis to see where each model struggles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "UAvwaCV3mEK9",
    "outputId": "a1db1fed-9b18-478c-d516-38775b5f47cc"
   },
   "source": [
    "Already installed:\n",
    "\n",
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install evaluate\n",
    "!pip install accelerate\n",
    "!pip install -U bitsandbytes\n",
    "!pip install torch\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3080 Ti\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEic4hEku7h0"
   },
   "source": [
    "## 1. Choose a Dataset\n",
    "You can pick: (or try some other ones you find interesting):\n",
    "- IMDB Movie Reviews (sentiment labels: positive/negative).\n",
    "- Yelp Reviews (sentiment labels: star ratings or binary positive/negative).\n",
    "\n",
    "For now, complete the rest of the steps (2-4) below with the above two datasets. Come back to do the following task after you're done with the above (time permitting):\n",
    "\n",
    "We are going to use the Amazon Product Reviews (various categories, can be collapsed into positive/negative) dataset. You are free to decide how to collapse multiple categories into one. You can also compare different approaches of this as well.\n",
    "\n",
    "Feel free to use the datasets library (e.g., load_dataset(\"imdb\")).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dlwIqTopjyhu",
    "outputId": "29fdcc07-b244-48f8-b9f8-eb90b3cfb523"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n",
      "I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn't match the background, and painfully one-dimensional characters cannot be overcome with a 'sci-fi' setting. (I'm sure there are those of you out there who think Babylon 5 is good sci-fi TV. It's not. It's clich√©d and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It's really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it's rubbish as they have to always say \"Gene Roddenberry's Earth...\" otherwise people would not continue watching. Roddenberry's ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.\n",
      "0\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 560000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 38000\n",
      "    })\n",
      "})\n",
      "Contrary to other reviews, I have zero complaints about the service or the prices. I have been getting tire service here for the past 5 years now, and compared to my experience with places like Pep Boys, these guys are experienced and know what they're doing. \\nAlso, this is one place that I do not feel like I am being taken advantage of, just because of my gender. Other auto mechanics have been notorious for capitalizing on my ignorance of cars, and have sucked my bank account dry. But here, my service and road coverage has all been well explained - and let up to me to decide. \\nAnd they just renovated the waiting room. It looks a lot better than it did in previous years.\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "imdb = load_dataset(\"imdb\")\n",
    "yelp = load_dataset(\"yelp_polarity\") # the name of the dataset is not 'yelp', it's 'yelp_polarity'\n",
    "\n",
    "imdb_texts = imdb[\"test\"][\"text\"]\n",
    "imdb_labels = imdb[\"test\"][\"label\"]\n",
    "\n",
    "yelp_texts = yelp[\"test\"][\"text\"]\n",
    "yelp_labels = yelp[\"test\"][\"label\"]\n",
    "\n",
    "print(imdb)\n",
    "print(imdb_texts[0])\n",
    "print(imdb_labels[0])\n",
    "print(yelp)\n",
    "print(yelp_texts[0])\n",
    "print(yelp_labels[0])\n",
    "\n",
    "# /usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning:\n",
    "# The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
    "# To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
    "# You will be able to reuse this secret in all of your notebooks.\n",
    "\n",
    "# what does this mean? API key?\n",
    "# the dataset is quite big so we should limit it to maybe 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1Ov5zMtPGHLx"
   },
   "outputs": [],
   "source": [
    "imdb_texts = imdb[\"test\"][\"text\"][:1000]\n",
    "imdb_labels = imdb[\"test\"][\"label\"][:1000]\n",
    "\n",
    "yelp_texts = yelp[\"test\"][\"text\"][:1000]\n",
    "yelp_labels = yelp[\"test\"][\"label\"][:1000]\n",
    "\n",
    "# choose sample 1000 because original datasets are too big \n",
    "# 1000 is a good number, not too small and doesnt run into long runtime + prevent GPU memory overflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHuMnpeFvDSY"
   },
   "source": [
    "## 2. Select Two (or More) Pretrained Models\n",
    "\n",
    "Pick at least two from the Hugging Face Hub and compare them:\n",
    "\n",
    "DistilBERT (e.g., distilbert-base-uncased-finetuned-sst-2-english)\n",
    "\n",
    "BERT (e.g., bert-base-uncased-finetuned-sst-2-english)\n",
    "\n",
    "RoBERTa (e.g., cardiffnlp/twitter-roberta-base-sentiment-latest or roberta-base-openai-detector)\n",
    "\n",
    "Feel free to explore the Hugging Face Model Hub if you find something else interesting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 669
    },
    "id": "D50BX2QAvGAB",
    "outputId": "c4e8a881-8552-48ea-ef12-87a79fe9a49c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# testing two different ways to load the models\n",
    "\n",
    "# method 1\n",
    "model1 = pipeline(\n",
    "    'sentiment-analysis', \n",
    "    model = 'distilbert-base-uncased-finetuned-sst-2-english', \n",
    "    truncation = True # added truncation because runtime error\n",
    "    )\n",
    "\n",
    "model2 = pipeline(\n",
    "    'sentiment-analysis', \n",
    "    model = 'cardiffnlp/twitter-roberta-base-sentiment-latest',\n",
    "    truncation = True, # added truncation because runtime error\n",
    "    padding = True, # added padding due to error with different lengths \n",
    "    max_length = 512 # ensure sequences are no longer than 512 tokens was getting error w/ Deepseek\n",
    "    )\n",
    "\n",
    "# migrated from colab -> jupyter notebook\n",
    "# bert-base-uncased-finetuned-sst-2-english is not available on HF? neither is \"bert-base-uncased-finetuned-sst-2-english\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 2 (chat GPT suggested)\n",
    "model1 = pipeline(\n",
    "    'sentiment-analysis', \n",
    "    model='distilbert-base-uncased-finetuned-sst-2-english',\n",
    "    tokenizer='distilbert-base-uncased-finetuned-sst-2-english',\n",
    "    device=0\n",
    ")\n",
    "\n",
    "model2 = pipeline(\n",
    "    'sentiment-analysis',\n",
    "    model='cardiffnlp/twitter-roberta-base-sentiment-latest',\n",
    "    tokenizer='cardiffnlp/twitter-roberta-base-sentiment-latest',\n",
    "    device=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gC8TTPHN6QBB",
    "outputId": "3d88163a-ebf3-445b-8a36-0c550ae31ef2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.pipelines.text_classification.TextClassificationPipeline object at 0x000001B542766280>\n",
      "<transformers.pipelines.text_classification.TextClassificationPipeline object at 0x000001B542764550>\n",
      "[{'label': 'POSITIVE', 'score': 0.999713122844696}]\n",
      "[{'label': 'NEGATIVE', 'score': 0.9991129040718079}]\n",
      "[{'label': 'positive', 'score': 0.9173767566680908}]\n",
      "[{'label': 'negative', 'score': 0.7887632846832275}]\n"
     ]
    }
   ],
   "source": [
    "# testing it \n",
    "print(model1)\n",
    "print(model2)\n",
    "print(model1(\"i love dogs\"))\n",
    "print(model1(\"i hate you\"))\n",
    "print(model2(\"i love dogs\"))\n",
    "print(model2(\"i hate you\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading model using method 1 and 2 yield same result:\n",
    "\n",
    "# <transformers.pipelines.text_classification.TextClassificationPipeline object at 0x0000014F60FA6760>\n",
    "# <transformers.pipelines.text_classification.TextClassificationPipeline object at 0x0000014F0013AF70>\n",
    "# [{'label': 'POSITIVE', 'score': 0.999713122844696}]\n",
    "# [{'label': 'NEGATIVE', 'score': 0.9991129040718079}]\n",
    "# [{'label': 'positive', 'score': 0.9173767566680908}]\n",
    "# [{'label': 'negative', 'score': 0.7887632846832275}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N9-jhwtIA0oS",
    "outputId": "490e5e2a-1b64-4b61-adbe-6358dd80a6fe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# import a progress bar\n",
    "\n",
    "# use BERT to predict sentiment analysis for both imdb and yelp datasets\n",
    "imdb_prediction1 = [1 if r[\"label\"] == \"POSITIVE\"\n",
    "    else 0 for r in tqdm(model1(imdb_texts, batch_size=32, truncation=True, padding=True))]\n",
    "\n",
    "yelp_prediction1 = [1 if r[\"label\"] == \"POSITIVE\"\n",
    "    else 0 for r in tqdm(model1(yelp_texts, batch_size=32, truncation=True, padding=True))]\n",
    "\n",
    "# RuntimeError: The size of tensor a (532) must match the size of tensor b (512) at non-singleton dimension 1, need to add truncation for model1 (BERT)\n",
    "# this takes forever to tokenize if doing linearly 1 by 1, maybe can batch by 32, and 16 if run into memory problems\n",
    "\n",
    "imdb_prediction2 = [1 if r[\"label\"] == \"positive\"\n",
    "    else 0 for r in tqdm(model2(imdb_texts, batch_size=32, truncation=True, padding=True))]\n",
    "\n",
    "yelp_prediction2 = [1 if r[\"label\"] == \"positive\"\n",
    "    else 0 for r in tqdm(model2(yelp_texts, batch_size=32, truncation=True, padding=True))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "26nntL6bHb5P",
    "outputId": "c6627cd9-dfcd-474c-b906-33b6675ee932",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(imdb_prediction1)\n",
    "print(yelp_prediction1)\n",
    "print('\\n')\n",
    "print(len(imdb_prediction1))\n",
    "\n",
    "# 1000 negative and positive reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check if yelp_prediction1 = yelp_prediction2, if not, compare the differences \n",
    "print(imdb_prediction2)\n",
    "print(yelp_prediction2)\n",
    "print('\\n')\n",
    "print(len(imdb_prediction2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_mismatches(array1, array2):\n",
    "    if len(array1) != len(array2):\n",
    "        raise ValueError(\"arrays must have the same length\")\n",
    "\n",
    "    # count mismatches\n",
    "    mismatch_count = sum(1 for a, b in zip(array1, array2) if a != b)\n",
    "\n",
    "    return mismatch_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(count_mismatches(imdb_prediction1, imdb_prediction2))\n",
    "print(count_mismatches(imdb_prediction1, imdb_prediction2)/1000)\n",
    "# 162 mismatches\n",
    "# 16.2 percent of mismatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VotMdHRgvGvp"
   },
   "source": [
    "## 3. Measure Performance\n",
    "Implement an evaluation method on a test or validation split. At minimum:\n",
    "- Accuracy: The fraction of examples predicted correctly.\n",
    "\n",
    "- F1 Score: Combination of precision and recall. (explanation of this is given below, after the instructions)\n",
    "You can use the Hugging Face evaluate or datasets library or write your own small function for computing these metrics.\n",
    "\n",
    "### F1 Score Explanation\n",
    "\n",
    "Imagine you‚Äôre trying to detect ‚Äúpositive‚Äù cases‚Äîfor example, emails that are spam. Your model‚Äôs predictions might fall into these categories:\n",
    "- True Positive (TP): Predicted spam, actually spam\n",
    "- False Positive (FP): Predicted spam, but it‚Äôs not spam\n",
    "- True Negative (TN): Predicted not spam, actually not spam\n",
    "- False Negative (FN): Predicted not spam, but it was spam\n",
    "\n",
    "Two important measures come from this:\n",
    "- Precision: Out of the emails you labeled spam, how many were actually spam? Precision=TP/(TP + FP)‚Äã\n",
    "- Recall: Out of the emails that were actually spam, how many did you catch? Recall=TP‚Äã/(TP + FN)\n",
    "\n",
    "But often, focusing on just Precision or just Recall is not enough. The F1 score combines both in a single number. It‚Äôs defined as the harmonic mean of Precision and Recall:\n",
    "\n",
    "F1=2√ó ((Precision√óRecall‚Äã)/(Precision+Recall))\n",
    "\n",
    "This way, if either Precision or Recall is low, the F1 score will also be relatively low.\n",
    "\n",
    "Example with a Small Confusion Matrix\n",
    "Suppose your model had these results:\n",
    "\n",
    "- TP = 4\n",
    "- FP = 2\n",
    "- FN = 1\n",
    "- TN = 3\n",
    "\n",
    "Then:\n",
    "\n",
    "- Precision=4/(4+2)‚Äã=0.66 (about 66%)\n",
    "- Recall=4/(4+1)‚Äã=0.80 (80%)\n",
    "\n",
    "So,\n",
    "F1=2√ó(0.66√ó0.80)/(0.66+0.80)‚Äã‚âà0.72\n",
    "\n",
    "Quick Python Example\n",
    "\n",
    "Below is a short snippet using sklearn (you dont have to usethis, hugging face also has a f1 function) to calculate the F1 score from some example predictions:\n",
    "\n",
    "\n",
    "```\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "True labels and model predictions\n",
    "y_true = [1, 1, 0, 1, 0, 0]\n",
    "y_pred = [1, 0, 0, 1, 0, 1]\n",
    "\n",
    "Calculate F1 score\n",
    "score = f1_score(y_true, y_pred)\n",
    "print(\"F1 Score:\", score)\n",
    "```\n",
    "\n",
    "If you run this, you‚Äôll see a single value that summarizes how good your predictions are at correctly identifying positives (with both ‚Äúhow often you‚Äôre correct‚Äù in positives, and ‚Äúhow many positives you caught‚Äù taken into account).\n",
    "\n",
    "Key Takeaways\n",
    "\n",
    "- F1 score balances Precision and Recall into one number.\n",
    "- If you need a single metric to judge performance in situations where both false positives and false negatives matter, F1 is often a good choice.\n",
    "- In Python, `sklearn.metrics.f1_score` makes it easy to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9d4tIeYy5rrk",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "\n",
    "def calculate_performance(predictions, labels):\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions)  \n",
    "    return accuracy, f1\n",
    "\n",
    "distilbert_imdb_accuracy, distilbert_imdb_f1 = calculate_performance(imdb_prediction1, imdb_labels)\n",
    "distilbert_yelp_accuracy, distilbert_yelp_f1 = calculate_performance(yelp_prediction1, yelp_labels)\n",
    "roberta_imdb_accuracy, roberta_imdb_f1 = calculate_performance(imdb_prediction2, imdb_labels)\n",
    "roberta_yelp_accuracy, roberta_yelp_f1 = calculate_performance(yelp_prediction2, yelp_labels)\n",
    "\n",
    "print(f\"DistilBERT IMDB Accuracy: {distilbert_imdb_accuracy}, F1: {distilbert_imdb_f1}\")\n",
    "print(f\"DistilBERT Yelp Accuracy: {distilbert_yelp_accuracy}, F1: {distilbert_yelp_f1}\")\n",
    "print(f\"RoBERTa IMDB Accuracy: {roberta_imdb_accuracy}, F1: {roberta_imdb_f1}\")\n",
    "print(f\"RoBERTa Yelp Accuracy: {roberta_yelp_accuracy}, F1: {roberta_yelp_f1}\")\n",
    "\n",
    "# F1 is producing 0? why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DistilBERT IMDB: \n",
    "- model is performing well on the negative class (precision: 1.00, recall: 0.91, F1: 0.95), the recall and f1-score are zero, which means DistilBERT is failing to predict any positive instances\n",
    "\n",
    "RoBERTa IMDB: \n",
    "- performs very well on the negative class (precision: 1.00, recall: 0.89, F1: 0.94), but fails to predict any positive instances (F1: 0.0)\n",
    "\n",
    "Need to fix this problem with IMDB dataset positive instances not getting detected\n",
    "\n",
    "DistilBERT Yelp:  \n",
    "- model is doing better on Yelp with both positive and negative classes showing reasonable performance (precision, recall, and F1 around 0.88 to 0.91), and  identifying positive and negative reviews \n",
    "\n",
    "RoBERTa Yelp: \n",
    "- lower performance on Yelp (accuracy: 0.86, F1: 0.86), but it's still within range, not as strong as DistilBERT on this dataset but performs decently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# raw predictions from DistilBERT on the first 10 IMDB reviews\n",
    "distilbert_raw_output = model1(imdb_texts[:10])\n",
    "\n",
    "# raw output to inspect the probabilities\n",
    "print(\"DistilBERT Raw Output (first 10 examples):\")\n",
    "for i, output in enumerate(distilbert_raw_output):\n",
    "    print(f\"Text: {imdb_texts[i]}\")\n",
    "    print('\\n')\n",
    "    print(f\"Prediction: {output['label']}, Confidence: {output['score']}\")\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "print ('----- line break -----')\n",
    "print('\\n')\n",
    "\n",
    "# raw predictions from RoBERTa on the first 10 IMDB reviews\n",
    "roberta_raw_output = model2(imdb_texts[:10])\n",
    "\n",
    "# raw output to inspect the probabilities\n",
    "print(\"\\nRoBERTa Raw Output (first 10 examples):\")\n",
    "for i, output in enumerate(roberta_raw_output):\n",
    "    print(f\"Text: {imdb_texts[i]}\")\n",
    "    print('\\n')\n",
    "    print(f\"Prediction: {output['label']}, Confidence: {output['score']}\")\n",
    "    print('\\n')\n",
    "    \n",
    "# the point of this is to check that these models are: \n",
    "# 1) actually outputting something\n",
    "# 2) if the output is upper/lower case\n",
    "# 3) how much confidence there is (potentially set a confidence threshold in the future thats acceptable)\n",
    "\n",
    "# we see that these raw outputs clearly exist so there must be some problem with function logic or \n",
    "# something potentially wrong with the model being trained for identifying positive instances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# maybe the model is not seeing \"POSITIVE\" and \"positive\" so hard code it to make sure it's .lower() and .upper()\n",
    "imdb_prediction1 = [\n",
    "    1 if r[\"label\"].lower() == \"positive\" \n",
    "    else 0 if r[\"label\"].lower() == \"negative\" \n",
    "    else None for r in tqdm(model1(imdb_texts, batch_size=32, truncation=True, padding=True))\n",
    "]\n",
    "\n",
    "yelp_prediction1 = [\n",
    "    1 if r[\"label\"].lower() == \"positive\" \n",
    "    else 0 if r[\"label\"].lower() == \"negative\" \n",
    "    else None for r in tqdm(model1(yelp_texts, batch_size=32, truncation=True, padding=True))\n",
    "]\n",
    "\n",
    "imdb_prediction2 = [1 if r[\"label\"].lower() == \"positive\" else 0 for r in model2(imdb_texts)]\n",
    "yelp_prediction2 = [1 if r[\"label\"].lower() == \"positive\" else 0 for r in model2(yelp_texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# copy paste code from above and try again\n",
    "def calculate_performance(predictions, true_labels):\n",
    "    # filter out None values (for neutral predictions) and align with actual labels\n",
    "    valid_predictions = [p for p in predictions if p is not None]\n",
    "    valid_labels = [true_labels[i] for i in range(len(predictions)) if predictions[i] is not None]\n",
    "    \n",
    "    accuracy = accuracy_score(valid_labels, valid_predictions)\n",
    "    f1 = f1_score(valid_labels, valid_predictions, zero_division=1)  # avoid undefined metric errors\n",
    "    return accuracy, f1\n",
    "\n",
    "distilbert_imdb_accuracy, distilbert_imdb_f1 = calculate_performance(imdb_prediction1, imdb_labels)\n",
    "distilbert_yelp_accuracy, distilbert_yelp_f1 = calculate_performance(yelp_prediction1, yelp_labels)\n",
    "roberta_imdb_accuracy, roberta_imdb_f1 = calculate_performance(imdb_prediction2, imdb_labels)\n",
    "roberta_yelp_accuracy, roberta_yelp_f1 = calculate_performance(yelp_prediction2, yelp_labels)\n",
    "\n",
    "print(f\"DistilBERT IMDB Accuracy: {distilbert_imdb_accuracy}, F1: {distilbert_imdb_f1}\")\n",
    "print(f\"DistilBERT Yelp Accuracy: {distilbert_yelp_accuracy}, F1: {distilbert_yelp_f1}\")\n",
    "print(f\"RoBERTa IMDB Accuracy: {roberta_imdb_accuracy}, F1: {roberta_imdb_f1}\")\n",
    "print(f\"RoBERTa Yelp Accuracy: {roberta_yelp_accuracy}, F1: {roberta_yelp_f1}\")\n",
    "\n",
    "# f1 still producing 0, so it can't be that the model is not recognizing the lower/upper case classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"DistilBERT Classification Report (IMDB):\")\n",
    "print(classification_report(imdb_labels, imdb_prediction1))\n",
    "\n",
    "print(\"RoBERTa Classification Report (IMDB):\")\n",
    "print(classification_report(imdb_labels, imdb_prediction2))\n",
    "\n",
    "print(\"DistilBERT Classification Report (Yelp):\")\n",
    "print(classification_report(yelp_labels, yelp_prediction1))\n",
    "\n",
    "print(\"RoBERTa Classification Report (Yelp):\")\n",
    "print(classification_report(yelp_labels, yelp_prediction2))\n",
    "\n",
    "# the classification report is showing that \"1\" is not being detected and that f1 is defaulting to 0.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# we can try balancing the dataset because the model might be overfitting? \n",
    "# balance dataset for 500 positive + negative instances (1000 samples from each class)\n",
    "imdb_pos = [x for x, y in zip(imdb[\"test\"][\"text\"], imdb[\"test\"][\"label\"]) if y == 1][:500]\n",
    "imdb_neg = [x for x, y in zip(imdb[\"test\"][\"text\"], imdb[\"test\"][\"label\"]) if y == 0][:500]\n",
    "imdb_texts_balanced = imdb_pos + imdb_neg\n",
    "imdb_labels_balanced = [1] * 500 + [0] * 500\n",
    "\n",
    "yelp_pos = [x for x, y in zip(yelp[\"test\"][\"text\"], yelp[\"test\"][\"label\"]) if y == 1][:500]\n",
    "yelp_neg = [x for x, y in zip(yelp[\"test\"][\"text\"], yelp[\"test\"][\"label\"]) if y == 0][:500]\n",
    "yelp_texts_balanced = yelp_pos + yelp_neg\n",
    "yelp_labels_balanced = [1] * 500 + [0] * 500\n",
    "\n",
    "# confirm that the final sizes after balance are 1000 still\n",
    "print(f\"IMDB Size: {len(imdb_texts_balanced)}\")\n",
    "print(f\"Yelp Size: {len(yelp_texts_balanced)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load both models\n",
    "model1 = pipeline(\n",
    "    'sentiment-analysis', \n",
    "    model='distilbert-base-uncased-finetuned-sst-2-english',\n",
    "    tokenizer='distilbert-base-uncased-finetuned-sst-2-english',\n",
    "    device=0\n",
    ")\n",
    "\n",
    "model2 = pipeline(\n",
    "    'sentiment-analysis',\n",
    "    model='cardiffnlp/twitter-roberta-base-sentiment-latest',\n",
    "    tokenizer='cardiffnlp/twitter-roberta-base-sentiment-latest',\n",
    "    device=0\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper to get predictions\n",
    "def get_predictions(model, texts):\n",
    "    preds = model(texts, batch_size=32, truncation=True, padding=True, max_length=512)\n",
    "    mapped = []\n",
    "    for r in preds:\n",
    "        label = r[\"label\"].lower()\n",
    "        if label == \"positive\":\n",
    "            mapped.append(1)\n",
    "        elif label == \"negative\":\n",
    "            mapped.append(0)\n",
    "        else:\n",
    "            mapped.append(None)  # neutral or unknown\n",
    "    return mapped\n",
    "\n",
    "# get predictions\n",
    "imdb_prediction1 = get_predictions(model1, imdb_texts_balanced)\n",
    "yelp_prediction1 = get_predictions(model1, yelp_texts_balanced)\n",
    "\n",
    "imdb_prediction2 = get_predictions(model2, imdb_texts_balanced)\n",
    "yelp_prediction2 = get_predictions(model2, yelp_texts_balanced)\n",
    "\n",
    "# \"You seem to be using the pipelines sequentially on GPU. \n",
    "# In order to maximize efficiency please use a dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation function\n",
    "def calculate_performance(predictions, labels, model_name, dataset_name):\n",
    "    valid = [(p, l) for p, l in zip(predictions, labels) if p is not None]\n",
    "    valid_preds, valid_labels = zip(*valid) if valid else ([], [])\n",
    "    if valid_preds:\n",
    "        acc = accuracy_score(valid_labels, valid_preds)\n",
    "        f1 = f1_score(valid_labels, valid_preds)\n",
    "        print(f\"{model_name} {dataset_name} | Accuracy: {acc:.3f} | F1: {f1:.3f}\")\n",
    "        print(classification_report(valid_labels, valid_preds, digits=3))\n",
    "    else:\n",
    "        print(f\"{model_name} {dataset_name} | No valid predictions.\")\n",
    "\n",
    "# results\n",
    "calculate_performance(imdb_prediction1, imdb_labels_balanced, \"DistilBERT\", \"IMDB\")\n",
    "calculate_performance(yelp_prediction1, yelp_labels_balanced, \"DistilBERT\", \"Yelp\")\n",
    "calculate_performance(imdb_prediction2, imdb_labels_balanced, \"RoBERTa\", \"IMDB\")\n",
    "calculate_performance(yelp_prediction2, yelp_labels_balanced, \"RoBERTa\", \"Yelp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# originally: \n",
    "\n",
    "# DistilBERT IMDB Accuracy: 0.908, F1: 0.0\n",
    "# DistilBERT Yelp Accuracy: 0.891, F1: 0.883919062832801\n",
    "# RoBERTa IMDB Accuracy: 0.888, F1: 0.0\n",
    "# RoBERTa Yelp Accuracy: 0.858, F1: 0.8582834331337326\n",
    "\n",
    "# previously, there was no positive sample present, f1 is calculated on how well you predict both 0 and 1\n",
    "# if we balance the dataset to support 500 positive and 500 negative examples, then the classifier is\n",
    "# asked to classify both classes during eval, so precision, recall, and f1 are all computable \n",
    "\n",
    "# f1 = 2 (precision)*(recall)/(precision + recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0rW2voE8vJ5G"
   },
   "source": [
    "## 4. Compare Models & Do a Short Error Analysis\n",
    "After running inference on your test set:\n",
    "- Compare Metrics: Which model is most accurate overall? Does one have higher F1?\n",
    "- Identify Edge Cases:\n",
    "\n",
    "  - Look at ~5 examples that were misclassified by at least one model.\n",
    "  - What patterns do you see? (e.g., tricky wording, sarcasm, short text, etc.) If you don't see any patter that's fine but make sure you've looked hard enough (maybe you need more than 5 examples?)\n",
    "  - What examples do all models make mistakes on? What mistakes are unique to a particular model? (again, if you dont find a patter that fine but make sure you've tried a lot of things)\n",
    "\n",
    "####  Write a short paragraph or make a small table summarizing your findings:\n",
    "\n",
    "- Which model performed best overall?\n",
    "- Any surprising differences?\n",
    "- How might you improve performance further?\n",
    "\n",
    "#### Deliverables\n",
    "- Code: A Python script or Jupyter notebook showing how you:\n",
    "  - Load data (and potentially preprocess it).\n",
    "  - Instantiate the Hugging Face pipelines.\n",
    "  - Run predictions and calculate metrics.\n",
    "\n",
    "- Short text on findings  ( in a text box in your Colab notebook):\n",
    "  - Which dataset(s) you chose and why.\n",
    "  - The models you compared and a table/plot of accuracy or F1 (use matplotlib for graphs).\n",
    "  - Example misclassified cases and your hypothesis for why they failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aaTPSehivKEg"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "models = ['DistilBERT', 'RoBERTa']\n",
    "datasets = ['IMDB', 'Yelp']\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)  # subplot for accuracy\n",
    "plt.bar(models, [distilbert_imdb_accuracy, roberta_imdb_accuracy], alpha=0.7, label=\"IMDB Accuracy\")\n",
    "plt.bar(models, [distilbert_yelp_accuracy, roberta_yelp_accuracy], alpha=0.7, label=\"Yelp Accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Model Comparison: Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "# f1 Score plot\n",
    "plt.subplot(1, 2, 2)  # subplot for f1 Score\n",
    "plt.bar(models, [distilbert_imdb_f1, roberta_imdb_f1], alpha=0.7, label=\"IMDB F1\")\n",
    "plt.bar(models, [distilbert_yelp_f1, roberta_yelp_f1], alpha=0.7, label=\"Yelp F1\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.title(\"Model Comparison: F1 Score\")\n",
    "plt.legend()\n",
    "\n",
    "# show plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t7j8FPYj30B2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNQNVhBEJS5KeGyY7XGbDuz",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "22f23175f9f44cdda4b6229f936bb1cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "252b1bb4b7c3484fac23ab875e4f6755": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "36759acb17d542d7919eb4d77cf45b6e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "48779259ce54415da1ee921ff98dfa25": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "54273996fb2640c3b54bc976e3d4a776": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_48779259ce54415da1ee921ff98dfa25",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b301ca104f1245fca0bd09c9df565a78",
      "value": 2
     }
    },
    "8d5daf6318d6481089733d457d390466": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c53a7183dc14a49b65b48380b16860a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aaedd6c01c634250ad6aeed3e45d19e7",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_22f23175f9f44cdda4b6229f936bb1cd",
      "value": "‚Äá2/2‚Äá[01:15&lt;00:00,‚Äá35.24s/it]"
     }
    },
    "9d717440b4bd49738d91b7d51b8b1758": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fd0497ece5c0470bb5c3159f996e79a4",
       "IPY_MODEL_54273996fb2640c3b54bc976e3d4a776",
       "IPY_MODEL_9c53a7183dc14a49b65b48380b16860a"
      ],
      "layout": "IPY_MODEL_36759acb17d542d7919eb4d77cf45b6e"
     }
    },
    "aaedd6c01c634250ad6aeed3e45d19e7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b301ca104f1245fca0bd09c9df565a78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fd0497ece5c0470bb5c3159f996e79a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8d5daf6318d6481089733d457d390466",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_252b1bb4b7c3484fac23ab875e4f6755",
      "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
