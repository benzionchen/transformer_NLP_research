{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1d2fae9",
   "metadata": {},
   "source": [
    "I’d like you to complete two parts:\n",
    "\n",
    "1. Part A: Data Loading & Splitting\n",
    "2. Part B: Tokenization & Preprocessing\n",
    "\n",
    "The goal is for you to get comfortable with Hugging Face’s datasets library for data manipulation and the tokenizer for preparing text for our language model. Unlike our previous assignment, we're going to now work on generative models instead of merely a classifier (think ChatGPT which produces new text instead of just saying yes/no like sentiment analysis). Below are the detailed instructions for each part. Take your time to read through why we do these steps, not just how to code them. By the end, you’ll have a nicely prepared dataset, ready for full training in a future assignment. This is just the setup, I'll send you more instructions in a couple of days on how to actually get to the finetuning part once I see how you guys are doing. Feel free to reach out if anything is unclear. Please push your code at regular intervals so that I can keep tabs. If you're stuck somewhere, try to reach out ASAP.\n",
    "\n",
    "These parts don't require a GPU to be used so, on Colab don't select a GPU for now. This will save your credits when we actually need them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1579797a",
   "metadata": {},
   "source": [
    "# Part A: Data Loading & Splitting\n",
    "\n",
    "## 1. Load the tiny_shakespeare Dataset\n",
    "Use the Hugging Face datasets library’s load_dataset function with \"tiny_shakespeare\" as the argument.\n",
    "Inspect the result to confirm you have splits named “train,” “validation,” and “test.” (don't worry about what these mean for now, we'll discuss them when we meet next time)\n",
    "Notice that each of these splits contains only 1 example (a single long string).\n",
    "\n",
    "## 2. Examine the Data\n",
    "Retrieve the string from the \"train\" split. (For example, you’ll see a dictionary with a key like \"text\"—that’s your single item.)\n",
    "Print out a small snippet (e.g., the first few hundred characters) to see how it looks. Notice it’s multiple lines of Shakespeare text, separated by \\n.\n",
    "\n",
    "## 3. Convert the Single Example into Multiple Lines\n",
    "You’ll need to split the long string using the newline character (\"\\n\").\n",
    "Remove any lines that are completely empty or just whitespace.\n",
    "Finally, you’ll have a list of lines—each line is a small piece of Shakespeare text.\n",
    "\n",
    "## 4. Create a Dataset of Lines\n",
    "Transform that list of lines into a Hugging Face Dataset object.\n",
    "This will give you a dataset with many rows (one row per line), rather than a single row with a giant string.\n",
    "\n",
    "## 5. Split That Dataset into Train & Validation\n",
    "Use the .train_test_split method (from the datasets library) on your newly created dataset.\n",
    "Choose a test size (like 0.1, or 10%). The result is a DatasetDict with a “train” split and a “test” split.\n",
    "Name them train_data and val_data (since we’re treating the test split as validation).\n",
    "Print out the sizes to confirm you have a healthy number of lines in each.\n",
    "At the end of doing this, you should have a script that ends with you having train_data and val_data each containing multiple lines of Shakespeare text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae3d59e",
   "metadata": {},
   "source": [
    "# Part B: Tokenization & Preprocessing\n",
    "\n",
    "For this part, I'm giving you most of the code. Your job is to basically glue this together. This will require some reading of the docs on your part so make sure to look up the functions I'm describing here and what they do. We'll talk about these a lot next time.\n",
    "\n",
    "## 1. Load the Model & Tokenizer\n",
    "What: We’ll use the model distilgpt2.\n",
    "Why: A pretrained tokenizer ensures we map text to the correct input IDs for our model (basically it maps text to numbers which computers can understand).\n",
    "Code Snippet:\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "GPT-2 doesn't have a pad token by default, so:\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "Expected: A loaded tokenizer that can handle Shakespeare text, turning each line into tokens.\n",
    "\n",
    "## 2. Write a tokenize_function\n",
    "What: A function that takes a batch of text lines, and returns their tokenized form. ML models take inputs in so-called \"batches\" - meaning that passing them one input at a time is wasteful, so usually multiple inputs are passed at once. When you see something like batch_size = 256, it means that the model takes in 256 inputs at the same time. Ideally of course we'd like to pass the entire dataset in a single batch but GPUs don't have enough memory to store the entire dataset so we need to pick a large enough batch size for efficiency but small enough to fit in your GPU. (if your batch size is too large for your GPU, you might get a GPU VRAM fault , think of it something like segmentation fault but for GPUs)\n",
    "Why: Hugging Face’s .map() calls this tokenizer on each batch behind the scenes. Essentially, for a batch of sentences, the tokenizer maps them to numbers.\n",
    "Key Points:\n",
    "We do truncation=True and max_length=128 or 256 for memory efficiency.\n",
    "Remove any lingering empty lines if needed.\n",
    "Code Snippet\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], \n",
    "        truncation=True, \n",
    "        max_length=128\n",
    "    )\n",
    "## 3. Apply .map() to Create train_dataset & val_dataset\n",
    "What: Convert your raw text lines into model-ready tokens. Don't worry about what tokens mean for now, we'll discuss this later. Might be a good idea to print these out to get a feel for what these are.\n",
    "Why: This is the final step before training. We remove the original “text” column, leaving only tokenized forms.\n",
    "train_dataset = train_data.map(tokenize_function, batched=True, remove_columns=[\"text\"]) \n",
    "val_dataset   = val_data.map(tokenize_function,   batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "Expected:\n",
    "Each sample in train_dataset now has keys like input_ids and attention_mask.\n",
    "Print train_dataset[0] to see the tokens if you wish.\n",
    "\n",
    "## 4. (Optional) Shuffle or Subset\n",
    "If the dataset is still large, you can do something like:\n",
    "train_dataset = train_dataset.shuffle(seed=42).select(range(2000))\n",
    "to keep training quick. That’s optional but recommended for demonstration.\n",
    "What you should have by now is a script that loads the split dataset from Part A, tokenizes it, and stores the final train_dataset and val_dataset.\n",
    "\n",
    "Some extra tasks after the above is done (only attempt after the above is done!!! Make sure to push your changes!):\n",
    "Chunking Variation for Part A\n",
    "Instead of splitting just by \\n, you might want to group lines into paragraphs or scenes if they’re dealing with Shakespeare.\n",
    "Design a small function that merges, say, 5 lines at a time into a single example, then compare how it changes the dataset size (compared to when you're spliting on a new line).\n",
    "Explore Tokenizer Settings in Part B\n",
    "For instance, do you want to enable padding=\"max_length\" vs. Padding=\"longest\"? Read up on what this means in the docs. Don't worry if you don't understand everything here.\n",
    "Print things out and investigate how that changes the shape of each batch.\n",
    "Light Analysis Before Training (After Part B is done)\n",
    "Look at average token length across examples or the distribution of line lengths in your dataset.\n",
    "Make a small table showing % of lines that exceed the chosen max_length (i.e., how many get truncated). This will help us decide on what max_length to choose. Overall, you should make a function which takes in max_length and shows a % of lines that exceed the chosen max_length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8e2ec1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
